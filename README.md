# WILDCHAT
IDMP Project Fall 2024

This project focusses on the WildCHAT datatset, which is a corpus of 1M chat interactions from anonymized users and chat gpt bots. The dataset is studied, to implement a keyword based search to find similar chat responses based on filter catgeories ( keyword, country, state , bot model ). Further the similar chat interactions are summarized to get a better understanding of the overall interaction from that particular user. A sample of the data set for this project is available in the GIT,  

##PREPROCESSING

1. The primary focus on conversations in this project will be the interactions in English language.

2. All toxic data is exceluded from the project scope.

3. The dataset contains the following schema : 
        root
       |-- conversation_hash: string (nullable = true)
       |-- model: string (nullable = true)
       |-- timestamp: timestamp (nullable = true)
       |-- conversation: array (nullable = true)
       |    |-- element: struct (containsNull = true)
       |    |    |-- content: string (nullable = true)
       |    |    |-- country: string (nullable = true)
       |    |    |-- hashed_ip: string (nullable = true)
       |    |    |-- header: struct (nullable = true)
       |    |    |    |-- accept-language: string (nullable = true)
       |    |    |    |-- user-agent: string (nullable = true)
       |    |    |-- language: string (nullable = true)
       |    |    |-- redacted: boolean (nullable = true)
       |    |    |-- role: string (nullable = true)
       |    |    |-- state: string (nullable = true)
       |    |    |-- timestamp: timestamp (nullable = true)
       |    |    |-- toxic: boolean (nullable = true)
       |    |    |-- turn_identifier: long (nullable = true)
       |-- turn: long (nullable = true)
       |-- language: string (nullable = true)
       |-- openai_moderation: array (nullable = true)
       |    |-- element: struct (containsNull = true)
       |    |    |-- categories: struct (nullable = true)
       |    |    |    |-- harassment: boolean (nullable = true)
       |    |    |    |-- harassment/threatening: boolean (nullable = true)
       |    |    |    |-- harassment_threatening: boolean (nullable = true)
       |    |    |    |-- hate: boolean (nullable = true)
       |    |    |    |-- hate/threatening: boolean (nullable = true)
       |    |    |    |-- hate_threatening: boolean (nullable = true)
       |    |    |    |-- self-harm: boolean (nullable = true)
       |    |    |    |-- self-harm/instructions: boolean (nullable = true)
       |    |    |    |-- self-harm/intent: boolean (nullable = true)
       |    |    |    |-- self_harm: boolean (nullable = true)
       |    |    |    |-- self_harm_instructions: boolean (nullable = true)
       |    |    |    |-- self_harm_intent: boolean (nullable = true)
       |    |    |    |-- sexual: boolean (nullable = true)
       |    |    |    |-- sexual/minors: boolean (nullable = true)
       |    |    |    |-- sexual_minors: boolean (nullable = true)
       |    |    |    |-- violence: boolean (nullable = true)
       |    |    |    |-- violence/graphic: boolean (nullable = true)
       |    |    |    |-- violence_graphic: boolean (nullable = true)
       |    |    |-- category_scores: struct (nullable = true)
       |    |    |    |-- harassment: double (nullable = true)
       |    |    |    |-- harassment/threatening: double (nullable = true)
       |    |    |    |-- harassment_threatening: double (nullable = true)
       |    |    |    |-- hate: double (nullable = true)
       |    |    |    |-- hate/threatening: double (nullable = true)
       |    |    |    |-- hate_threatening: double (nullable = true)
       |    |    |    |-- self-harm: double (nullable = true)
       |    |    |    |-- self-harm/instructions: double (nullable = true)
       |    |    |    |-- self-harm/intent: double (nullable = true)
       |    |    |    |-- self_harm: double (nullable = true)
       |    |    |    |-- self_harm_instructions: double (nullable = true)
       |    |    |    |-- self_harm_intent: double (nullable = true)
       |    |    |    |-- sexual: double (nullable = true)
       |    |    |    |-- sexual/minors: double (nullable = true)
       |    |    |    |-- sexual_minors: double (nullable = true)
       |    |    |    |-- violence: double (nullable = true)
       |    |    |    |-- violence/graphic: double (nullable = true)
       |    |    |    |-- violence_graphic: double (nullable = true)
       |    |    |-- flagged: boolean (nullable = true)
       |-- detoxify_moderation: array (nullable = true)
       |    |-- element: struct (containsNull = true)
       |    |    |-- identity_attack: double (nullable = true)
       |    |    |-- insult: double (nullable = true)
       |    |    |-- obscene: double (nullable = true)
       |    |    |-- severe_toxicity: double (nullable = true)
       |    |    |-- sexual_explicit: double (nullable = true)
       |    |    |-- threat: double (nullable = true)
       |    |    |-- toxicity: double (nullable = true)
       |-- toxic: boolean (nullable = true)
       |-- redacted: boolean (nullable = true)
       |-- state: string (nullable = true)
       |-- country: string (nullable = true)
       |-- hashed_ip: string (nullable = true)
       |-- header: struct (nullable = true)
       |    |-- accept-language: string (nullable = true)
       |    |-- user-agent: string (nullable = true)
   Of which, Header, detoxify_moderation, and openai_moderation fields are dropped to avoid using any toxic data set and to preserve a clarity in the dataset.

4. To better understand the user / bot interactions , the conversation field is explored.
       conversation: array (nullable = true)
       |    |-- element: struct (containsNull = true)
       |    |    |-- content: string (nullable = true)
       |    |    |-- country: string (nullable = true)
       |    |    |-- hashed_ip: string (nullable = true)
       |    |    |-- header: struct (nullable = true)
       |    |    |    |-- accept-language: string (nullable = true)
       |    |    |    |-- user-agent: string (nullable = true)
       |    |    |-- language: string (nullable = true)
       |    |    |-- redacted: boolean (nullable = true)
       |    |    |-- role: string (nullable = true)
       |    |    |-- state: string (nullable = true)
       |    |    |-- timestamp: timestamp (nullable = true)
       |    |    |-- toxic: boolean (nullable = true)
       |    |    |-- turn_identifier: long (nullable = true)
