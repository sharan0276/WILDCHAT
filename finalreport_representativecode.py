# -*- coding: utf-8 -*-
"""FinalReport_RepresentativeCode.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OjiuueqDJek_sP_oQ1KVxCi0RxhaEurK
"""

!pip install transformers

import numpy as np
import pandas as pd
#import matplotlib.pyplot as plt
#import seaborn as sns
#import os

import fastparquet
from fastopic import FASTopic

#import re   # Import regular expressions
#import nltk
#from nltk.corpus import stopwords
#from topmost.preprocessing import Preprocessing

pd.set_option('display.max_colWidth', None)

from transformers import pipeline

df0 = fastparquet.ParquetFile('/content/part-00000-dfedd745-fcf6-48b5-b338-4cb51738d1b6-c000.snappy.parquet').to_pandas()
df1 = fastparquet.ParquetFile('/content/part-00001-dfedd745-fcf6-48b5-b338-4cb51738d1b6-c000.snappy.parquet').to_pandas()

df2 = fastparquet.ParquetFile('/content/part-00002-dfedd745-fcf6-48b5-b338-4cb51738d1b6-c000.snappy.parquet').to_pandas()
df3 = fastparquet.ParquetFile('/content/part-00003-dfedd745-fcf6-48b5-b338-4cb51738d1b6-c000.snappy.parquet').to_pandas()

df4 = fastparquet.ParquetFile('/content/part-00004-dfedd745-fcf6-48b5-b338-4cb51738d1b6-c000.snappy.parquet').to_pandas()
df5 = fastparquet.ParquetFile('/content/part-00005-dfedd745-fcf6-48b5-b338-4cb51738d1b6-c000.snappy.parquet').to_pandas()

df6 = fastparquet.ParquetFile('/content/part-00006-dfedd745-fcf6-48b5-b338-4cb51738d1b6-c000.snappy.parquet').to_pandas()
df7 = fastparquet.ParquetFile('/content/part-00007-dfedd745-fcf6-48b5-b338-4cb51738d1b6-c000.snappy.parquet').to_pandas()

df8 = fastparquet.ParquetFile('/content/part-00008-dfedd745-fcf6-48b5-b338-4cb51738d1b6-c000.snappy.parquet').to_pandas()
df9 = fastparquet.ParquetFile('/content/part-00009-dfedd745-fcf6-48b5-b338-4cb51738d1b6-c000.snappy.parquet').to_pandas()

df10 = fastparquet.ParquetFile('/content/part-00010-dfedd745-fcf6-48b5-b338-4cb51738d1b6-c000.snappy.parquet').to_pandas()
df11 = fastparquet.ParquetFile('/content/part-00011-dfedd745-fcf6-48b5-b338-4cb51738d1b6-c000.snappy.parquet').to_pandas()

df12 = fastparquet.ParquetFile('/content/part-00012-dfedd745-fcf6-48b5-b338-4cb51738d1b6-c000.snappy.parquet').to_pandas()
df13 = fastparquet.ParquetFile('/content/part-00013-dfedd745-fcf6-48b5-b338-4cb51738d1b6-c000.snappy.parquet').to_pandas()

df14 = fastparquet.ParquetFile('/content/part-00014-dfedd745-fcf6-48b5-b338-4cb51738d1b6-c000.snappy.parquet').to_pandas()

df = pd.concat([df0,df1,df2,df3,df4,df5]) #,df6,df7,df8,df9,df10,df11,df12,df13,df14
print(df.shape)

df.reset_index(inplace=True)

df.head(3)

# BART LLM for Text Summarization

summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

text = 'write a statement that shows the schedule, with the itv1 programming continuing until 4:30 pm, where kikoriki is the first new citv show to air, also list the itv1 programming that will air before 4:30 pm for immediate releasecitv unveils schedule leading up to grand unveiling at 4:30 pm kikoriki set to debut as first new citv show!city, date citv, the beloved childrens television channel, is excited to reveal the exhilarating schedule leading up to the grand unveiling at 4:30 pm on monday . the revamped citv promises an unforgettable viewing experience, and young audiences are in for a treat!starting from the temporary simulcast arrangement with itv1 at 6 am on saturday, citv will continue to simulcast itv1 programming, ensuring children do not miss out on their favorite shows . the lineup features a diverse range of engaging programs curated to captivate their imagination and provide entertainment until the grand unveiling . as the clock ticks closer to monday at 4:30 pm, citv will air a collection of exciting itv1 shows that will keep young viewers engaged and entertained . notable highlights before the grand unveiling include: show 1 at time welcome the day with this beloved series that has captivated audiences with its heartwarming stories and delightful characters . show 2 at time prepare to embark on a thrilling adventure filled with action, humor, and valuable life lessons . show 3 at time join a group of young detectives as they solve mysteries and outsmart villains with their intelligence and teamwork . finally, at 4:30 pm, the eagerly awaited moment arrives the grand unveiling of the transformed citv . this monumental occasion marks the debut of the highly anticipated new show, kikoriki, as the first program airing on the refreshed channel . kikoriki, with its imaginative storytelling and lovable characters, is certain to mesmerize young viewers and set the stage for an exciting new era of citv . citv extends its heartfelt gratitude to its loyal viewers for their continued support and patience during the transitional period . with the enticing lineup of itv1 shows and the introduction of kikoriki, citv promises an unforgettable experience for children, sparking their imaginations and keeping them entertained . for further updates and information, please visit citvs official website or follow their social media channels . ###media contact:nametitleemailphone '

# 21 seconds to run
print(summarizer(text, max_length=120, do_sample=False)) #min_length=30,

# Better summary than 1 sentence summary from Xsum LLM
text = "write an article out of this information also come up with a new title: Title: Parts of Ramla evacuated after metal object detected in the sand Part of Ramla bay in Gozo was temporarily evacuated, when someone who was digging up the sand came across an unidentified metal object. The incident took place at about 14.00 hrs and Police and Armed Forces were called in to establish the precise nature of the object. As a precaution, people were asked to leave the area but it transpired that the object was a metal object buried in the sand."

# Better summary, runs longer
print(summarizer(text, max_length=50, min_length=20, do_sample=False)) #

# Testing BART large xsum model instead of cnn model, this outputs only 1 sentence summaries

summarizer = pipeline("summarization", model="facebook/bart-large-xsum")

text = "write an article out of this information also come up with a new title: Title: Parts of Ramla evacuated after metal object detected in the sand Part of Ramla bay in Gozo was temporarily evacuated, when someone who was digging up the sand came across an unidentified metal object. The incident took place at about 14.00 hrs and Police and Armed Forces were called in to establish the precise nature of the object. As a precaution, people were asked to leave the area but it transpired that the object was a metal object buried in the sand."

print(summarizer(text, min_length=10, do_sample=False)) #max_length=30,

text = 'what did he do mansa musa was a medieval african ruler of the mali empire in the 14 th century . he is known for his wealth and his pilgrimage to mecca, which brought the attention of the world to the empire . he promoted the spread of islam throughout mali and expanded the empires borders, taking over neighboring kingdoms and strengthening trade routes . during his reign, timbuktu became a major center of learning, attracting scholars from all over the islamic world . mansa musa is also known for his philanthropy and for the large sums of gold he distributed on his pilgrimage, which contributed to the inflation of gold prices and the devaluation of currency in the regions he passed through'


# Mecca is the holy city, seems to be misplaced with Timbuktu
print(summarizer(text, min_length=10, do_sample=False)) #max_length=30,

text = f'''Title:The first quarter of 2023 was the deadliest for migrants crossing the Mediterranean since 2017, with hundreds of lives lost trying to reach Europe, the UN said on Wednesday.

The United Nations' International Organization for Migration (IOM) estimated its figure of 441 deaths does not reflect the actual number of deaths in the sea, with many unknown and unaccounted for.

"With over 20,000 deaths recorded on this route since 2014, I fear that these deaths have been normalised," IOM warned, adding "delays and gaps in state-led search and rescue (SAR) operations are costing lives."


The UN body said delays in SAR operations have been a determining factor in at least six incidents since the beginning of the year, resulting in the deaths of at least 127 of the 441 people.

"The total lack of response during a seventh rescue operation cost the lives of at least 73 migrants," it said, pointing out that rescue efforts by NGOs have declined significantly in recent months.

Libya's Coast Guard has also reportedly fired shots towards humanitarian vessels to stop them from rescuing ships in distress.

"The continuing humanitarian crisis in the central Mediterranean is intolerable," IOM chief Antonio Vitorino said.


In March, Italy's Coast Guard was accused of deliberately delaying a rescue, leading to the deaths of 30 people. It denies this allegation saying the migrant ship was outside of its area of responsibility.
The UN agency's Missing Migrants Project is also investigating several cases of missing boats, where there are no traces of survivors, no debris and no SAR rescue operations have been carried out.

Some 300 people on board such boats are still missing, the organisation said.

"Saving lives at sea is a legal obligation for states," Vitorino continued.

"We need proactive coordination of States in search and rescue efforts. Guided by the spirit of shared responsibility and solidarity, we call on States to work together and strive to reduce the loss of life along migration routes."

The number of migrants trying to enter Italy has tripled compared to last year, simultaneously causing an increase in the number of maritime accidents.

A vessel carrying roughly 800 people on board was rescued on 11 April, more than 200 kilometres southeast of Sicily by the Italian Coast Guard with the assistance of a commercial vessel.

Another ship with around 400 migrants was reportedly adrift between Italy and Malta for two days before being reached the Italian Coast.

Not all migrants from these ships have reached safety and disembarked in Italy yet.'''

# Only 18 sec to run with xsum model, python f string works fine there
print(summarizer(text, min_length=10, do_sample=False)) #min_length=30,

text = 'come up with static variables for a class to handle and calculate complex numbers in c here are a few static variables that can be included in a class to handle and calculate complex numbers in c:1 . realpart: stores the real part of the complex number . double type2 . imaginarypart: stores the imaginary part of the complex number . double type3 . numofcomplexnumbers: keeps track of the number of complex numbers currently created . int type4 . defaultrealpart: represents the default value for the real part of a complex number . const double type5 . defaultimaginarypart: represents the default value for the imaginary part of a complex number . const double typeheres an example implementation:cppclass complexnumber static double realpart static double imaginarypart static int numofcomplexnumbers static constexpr double defaultrealpart = 0 . 0 static constexpr double defaultimaginarypart = 0 . 0 class methods and constructors . . . initializing static variablesdouble complexnumber::realpart = complexnumber::defaultrealpartdouble complexnumber::imaginarypart = complexnumber::defaultimaginarypartint complexnumber::numofcomplexnumbers = 0 note: the actual implementation may vary based on the specific requirements and functionality of the complex number class . the above example serves as a starting point and can be expanded upon as needed'

print(summarizer(text, max_length=40, min_length=10, do_sample=False)) #

text = 'find the inverse of the following function . fx=5 x24 x0 to find the inverse of the function fx = 5 x^2 4 x, we need to solve for x instead of y . step 1: replace fx with y:y = 5 x^2 4 xstep 2: interchange the x and y variables:x = 5 y^2 4 ystep 3: rearrange the equation to solve for y:0 = 5 y^2 4 y xstep 4: solve the quadratic equation for y using the quadratic formula:y = 4 4^2 45 x 25 simplifying further:y = 4 16 20 x 10 thus, the inverse of the function fx = 5 x^2 4 x is given by:f^1 x = 4 16 20 x 10'

# 11 sec to run
print(summarizer(text, max_length=40, min_length=10, do_sample=False)) #

text = 'generate an etsy title for a art work that can be added to mugs, painting, tshirt, etc within 140 characters with no & only | to separate without first word being whimsical and is the name of the art coffee in shop with flower coffee in shop with flower | versatile art for mugs, paintings, tshirts | unique artistic design for home and apparel'

# 11 sec to run
print(summarizer(text, max_length=40, min_length=10, do_sample=False)) #

# For Bart-cnn model, comparing to xsum
text = 'come up with static variables for a class to handle and calculate complex numbers in c here are a few static variables that can be included in a class to handle and calculate complex numbers in c:1 . realpart: stores the real part of the complex number . double type2 . imaginarypart: stores the imaginary part of the complex number . double type3 . numofcomplexnumbers: keeps track of the number of complex numbers currently created . int type4 . defaultrealpart: represents the default value for the real part of a complex number . const double type5 . defaultimaginarypart: represents the default value for the imaginary part of a complex number . const double typeheres an example implementation:cppclass complexnumber static double realpart static double imaginarypart static int numofcomplexnumbers static constexpr double defaultrealpart = 0 . 0 static constexpr double defaultimaginarypart = 0 . 0 class methods and constructors . . . initializing static variablesdouble complexnumber::realpart = complexnumber::defaultrealpartdouble complexnumber::imaginarypart = complexnumber::defaultimaginarypartint complexnumber::numofcomplexnumbers = 0 note: the actual implementation may vary based on the specific requirements and functionality of the complex number class . the above example serves as a starting point and can be expanded upon as needed'

print(summarizer(text, max_length=40, min_length=10, do_sample=False)) #

# Summarization of Just chatgpt 400 word rewrite of the electronics article for Xsum model
text='High-speed cameras have become increasingly important over the past few decades, with advancements in technology resulting in the development of even faster and more sophisticated models. High-speed cameras capture images and video sequences at exceptionally high frame rates, far faster than what traditional cameras are capable of. These cameras are commonly used in scientific research, industrial applications, and even in the entertainment industry.\n\nUnlike traditional cameras, high-speed cameras have advanced optical and image processing capabilities that enable them to capture images with unparalleled clarity and detail. Their lenses can zoom in on objects too small or fast for the naked eye to identify. High-speed cameras are equipped with sophisticated image processing software that allows for easy analysis of recorded footage, enabling researchers to gain deeper insights into their subject matter.\n\nThe use of high-speed cameras has become increasingly important in various fields, including industrial production, scientific research, military development, and media production. In industrial production, high-speed cameras monitor manufacturing processes, collision reports, detect defects, and ensure quality control. In scientific research, they study high-speed phenomena such as fluid dynamics and the behavior of biological specimens. In military development, high-speed cameras evaluate the performance of explosives and ballistic testing. In media production, high-speed cameras capture dramatic slow-motion footage of action scenes, stunts, and other visually stunning moments.\n\nPotential customers of high-speed cameras are vast, ranging from research institutions and manufacturing facilities to military organizations and film studios. High-end models can cost upwards of $100,000, making them a significant investment for many organizations.\n\nIn conclusion, high-speed cameras are an invaluable tool for a wide range of applications. The ability to capture high-speed phenomena with unparalleled clarity and detail is essential for making new discoveries, ensuring quality control, and creating stunning visual effects. As technology continues to advance, we can expect to see even faster, more sophisticated high-speed cameras that will help us explore new frontiers and gain deeper insights into the world around us'

# Only 13 sec to run
print(summarizer(text, min_length=10, do_sample=False)) #



from transformers import RobertaForSequenceClassification, RobertaTokenizer
from transformers.data.metrics import acc_and_f1, simple_accuracy

from transformers import TextClassificationPipeline

CODEBERTA_LANGUAGE_ID = "huggingface/CodeBERTa-language-id"
#CODEBERTA_PRETRAINED = "huggingface/CodeBERTa-small-v1"

pipeline = TextClassificationPipeline(
    model=RobertaForSequenceClassification.from_pretrained(CODEBERTA_LANGUAGE_ID),
    tokenizer=RobertaTokenizer.from_pretrained(CODEBERTA_LANGUAGE_ID)
)

test_code="""
def f(x):
    return x**2
"""
pipeline(test_code)

pipeline("Which programming language is this, python or not? const foo = 'bar'")

pipeline("echo $FOO Which programming language is this, python or not? ")

test="The python is a snake. outcome := rand.Intn(6) + 1"
pipeline(test)
# wrong should be language Go

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/DeepSeek-Coder-V2-Lite-Base", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("deepseek-ai/DeepSeek-Coder-V2-Lite-Base", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()
input_text = "#write a quick sort algorithm"
inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_length=128)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

# Deep Seeker Model takes +42GB space and fails on install due to mismatch in GPU, not Nvidia which cuda expects

# Searching out Wildchat conversations that are highly likely coding, to use with DeepSeek

searchterms = pd.Series(['algorithm','python','html'])
dfcsv=df[(df['state']=='California')]

dfcsv['article'] = dfcsv['rare_terms'].map(lambda x : sum(searchterms.isin(x)))
dfcsv.article.value_counts()

dfcsv[dfcsv['article'] > 1].tail()

# DeepSeek Coder V2 best state of the art code generation

# Prompt: write a quick sort algorithm
def quicksort(arr):
    if len(arr) <= 1:
        return arr
    else:
        pivot = arr[len(arr) // 2]
        left = [x for x in arr if x < pivot]
        middle = [x for x in arr if x == pivot]
        right = [x for x in arr if x > pivot]
        return quicksort(left) + middle + quicksort(right)

# Example usage:
arr = [3, 6, 8, 10, 1, 2, 1]
sorted_arr = quicksort(arr)
print(sorted_arr)

"""Explanation:
Base Case: If the array has 1 or 0 elements, it is already sorted, so we return it as is.

Pivot Selection: We choose a pivot element. In this implementation, the pivot is the middle element of the array.

Partitioning:

left: Elements less than the pivot.

middle: Elements equal to the pivot.

right: Elements greater than the pivot.

Recursive Sorting: We recursively apply the quicksort function to the left and right partitions.

Combine: Finally, we combine the sorted left partition, the middle partition, and the sorted right partition to get the final sorted array.

This implementation is simple and easy to understand, but it uses additional memory for the left, middle, and right lists. If you want an in-place version of QuickSort, you can modify the algorithm to sort the array directly without creating additional lists.
"""

# DeepSeek Coder V2 best state of the art code generation

# Prompt: write Dijkstra's algorithm

import heapq

def dijkstra(graph, start):
    # Initialize distances with infinity and the start node with 0
    distances = {node: float('infinity') for node in graph}
    distances[start] = 0

    # Priority queue to store (distance, node) pairs
    priority_queue = [(0, start)]

    while priority_queue:
        # Get the node with the smallest distance
        current_distance, current_node = heapq.heappop(priority_queue)

        # If the current distance is greater than the known distance, skip
        if current_distance > distances[current_node]:
            continue

        # Explore neighbors
        for neighbor, weight in graph[current_node].items():
            distance = current_distance + weight

            # If a shorter path is found, update the distance and push to the queue
            if distance < distances[neighbor]:
                distances[neighbor] = distance
                heapq.heappush(priority_queue, (distance, neighbor))

    return distances

# Example usage:
graph = {
    'A': {'B': 1, 'C': 4},
    'B': {'A': 1, 'C': 2, 'D': 5},
    'C': {'A': 4, 'B': 2, 'D': 1},
    'D': {'B': 5, 'C': 1}
}

"""Explanation:
Initialization:

distances: A dictionary to store the shortest distance from the start node to each node. Initially, all distances are set to infinity except for the start node, which is set to 0.

priority_queue: A min-heap to store (distance, node) pairs. Initially, it contains the start node with a distance of 0.

Main Loop:

While the priority queue is not empty, extract the node with the smallest distance.

If the extracted distance is greater than the known distance to that node, skip it (this can happen due to the nature of the heap).

For each neighbor of the current node, calculate the potential new distance.

If this new distance is shorter than the currently known distance to the neighbor, update the distance and push the new (distance, neighbor) pair onto the priority queue.

Result:

The distances dictionary will contain the shortest distance from the start node to each node in the graph.
"""

# DeepSeek Coder V2 best state of the art code generation, using DeepSeek website for manual prompts to model

# Prompt: write Bellman Ford's algorithm

def bellman_ford(graph, start):
    # Initialize distances with infinity and the start node with 0
    distances = {node: float('infinity') for node in graph}
    distances[start] = 0

    # Relax edges |V| - 1 times
    for _ in range(len(graph) - 1):
        for node in graph:
            for neighbor, weight in graph[node].items():
                if distances[node] + weight < distances[neighbor]:
                    distances[neighbor] = distances[node] + weight

    # Check for negative weight cycles
    for node in graph:
        for neighbor, weight in graph[node].items():
            if distances[node] + weight < distances[neighbor]:
                raise ValueError("Graph contains a negative weight cycle")

    return distances

# Example usage:
graph = {
    'A': {'B': -1, 'C': 4},
    'B': {'C': 3, 'D': 2, 'E': 2},
    'C': {},
    'D': {'B': 1, 'C': 5},
    'E': {'D': -3}
}

start_node = 'A'
shortest_distances = bellman_ford(graph, start_node)
print(shortest_distances)

"""Explanation:
Initialization:

distances: A dictionary to store the shortest distance from the start node to each node. Initially, all distances are set to infinity except for the start node, which is set to 0.

Relaxation:

The algorithm relaxes each edge |V| - 1 times, where |V| is the number of vertices in the graph. This is because in the worst case, the shortest path from the source to any node can have at most |V| - 1 edges.

For each edge (u, v) with weight w, if the distance to u plus w is less than the distance to v, update the distance to v.

Negative Weight Cycle Detection:

After |V| - 1 relaxations, if any edge can still be relaxed, it indicates the presence of a negative weight cycle. In this case, the algorithm raises an error.

Result:

The distances dictionary will contain the shortest distance from the start node to each node in the graph, assuming there are no negative weight cycles.
"""



# Representative Code for Spark and Flask UI

import findspark
findspark.init()
findspark.find()

#importing all essentail libraries for the processing
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.context import SparkContext
from pyspark.sql.types import ArrayType, StringType, BooleanType

spark = SparkSession.builder.appName('WildCHAT1M').config("spark.driver.memory", "24g").config("spark.executor.memory", "8g").config("spark.sql.debug.maxToStringFields", 1000).config("spark.executor.extraJavaOptions", "-XX:+UseG1GC").config("spark.executor.cores", "4").getOrCreate()

#Added more driver memory to help with chat aggregations and increased the Sql String Length to accomodate the larger chat and response strings.

spark_df = spark.read.parquet("/Users/sharan/Desktop/IDMP Data/*.parquet")

spark_df = spark_df.filter(F.col('redacted') == False).filter(F.col('toxic') == False).filter(F.col('language') == 'English')
#Saving records that are of English Language, do not contain any redacted information and are non - toxic.



spark_df.count()

spark_df.select('language').distinct().collect()  #Confirming only English language interactions are available

spark_df.select('redacted').distinct().collect() #Confirming only non - redacted interactions are available

spark_df.select('toxic').distinct().collect()   #Confirming only non - toxic interactions are available

spark_df.printSchema()

#Dropping columns openai_moderation, detoxify_moderation, and header for cleaner schema.
spark_df = spark_df.drop("openai_moderation", "detoxify_moderation", "header")

"""Schema for Conversation Field is made up of an array of structures, where each structure contains each user - bot interaction. The individual details of each interaction is printed in the schema above:
 * <b> content </b> contains the prompt / response
 * <b> role </b> specifies if the content present is generated from the user or by the bot
 * <b> turn_identifier </b> specifies an identifying value to isolate a specific chat interaction.

Exploding the conversation field to seperate the user prompts, bot responses.
"""



main_df = spark_df

#Isolating just the content and turn_identifier from each structure.
spark_df = spark_df.withColumn('exploded_conversation', F.explode(F.col('conversation'))).withColumn("content", F.col("exploded_conversation.content")).withColumn("turn_identifier",F.col("exploded_conversation.turn_identifier"))

spark_df.filter(F.col('turn') == 2).limit(4).show()

spark_df = spark_df.drop("exploded_conversation")

#Combining prompt and response based on turn_idenfier
#Recording the list of columns except for prompt to group rows based on other field values
group_cols = [col for col in spark_df.columns if col != 'content']
group_cols

spark_df = spark_df.groupBy(group_cols).agg(F.collect_list('content').alias('content'))

group_cols.remove('turn_identifier')
group_cols

pip install langid

import langid

# To save content with just english texts and not anything else check using
# import langid
# langid.classify(text)
# later apply spark filter to filter out interactions that do not belong to English language.

@F.udf(BooleanType())
def languageCheck(list):
    if list:
        lang, cnfLvl =  langid.classify(list[0])
        return lang == 'en'
    return False

spark_df.persist()

spark_df.filter(languageCheck(F.col('content')) == True).count()

spark_df = spark_df.filter(languageCheck(F.col('content')) == True)

#Combining interactions from single user
#Removing turn_identifier from group_cols to combine user interactions in prompt field and save turn identifiers.
spark_df = spark_df.groupBy(group_cols).agg(F.collect_list('content').alias('content'), F.collect_list('turn_identifier').alias('turn_identifier'))

#Defining user-defined Function to isolate user prompt
@F.udf(ArrayType(StringType()))
def get_user_prompt(list):
    if list:
        return [prompt for turn,conversation in enumerate(list) for interaction_turn, prompt in enumerate(conversation) if interaction_turn == 0]
    return []

#Defining user-defined Function to isolate bot response
@F.udf(ArrayType(StringType()))
def get_bot_response(list):
    if list:
        return [prompt for turn,conversation in enumerate(list) for interaction_turn, prompt in enumerate(conversation) if interaction_turn == 1]
    return []

spark_df = spark_df.withColumn('userprompt', get_user_prompt(F.col("content"))).withColumn('botresp', get_bot_response(F.col("content")))

#Defining a userdefined function to check if userprompt is only in english
@F.udf(StringType())
def userPromptCheck(list):
    if list:
          ln = "en"
          for text in list:
              lang, conflvl = langid.classify(text)
              if lang != "en":
                  ln = lang
          return ln
    return "na"

spark_df.withColumn('promptLanguage', userPromptCheck(F.col('userprompt'))).select('promptLanguage').distinct().collect()

#There are just english prompts in the dataset and they are about 454,931 records.

#Now that the dataset is updated to match the requirements, the user prompts and bot responses will be processed further to reduce their
#length and feed them as input to NLP models.

import nltk
from nltk.corpus import stopwords

from nltk.corpus import stopwords # importing to remove all stop words that are not significant from user prompt and bot responses
import re   # Import regular expressions

stop_words  = stopwords.words('english')

@F.udf(ArrayType(StringType()))
def preprocess(content):
    if content:
        updated_content = []
        for text in content :
            text = text.lower() #setting entire text to lower case
            text = re.sub(r'\s{2}+', '', text).strip() #removing white spaces
            text = re.sub(r'[^A-Za-z0-9\s$#@?.]', '', text) #removing any other speacial characters other than ones in the square bracket
            words = text.split()
            #print(words , "\n")
            filtered_words = [word for word in words if len(word) > 3 or (len(word) <= 3 and word not in stop_words)]
            #print(filtered_words)
            text = " ".join(filtered_words)
            updated_content.append(text)
        return updated_content
    return []

content = ["I     want to visit the city!     #excited", "This is a short test."]
processed_content = preprocess(content)
print(processed_content)

spark_df.withColumn('userprompt_up', preprocess(F.col('userprompt'))).withColumn('botresp_up', preprocess(F.col('botresp'))).first()

spark_df = spark_df.withColumn('userprompt_up', preprocess(F.col('userprompt'))).withColumn('botresp_up', preprocess(F.col('botresp')))

spark_df.show()

output_directory = "/Users/sharan/Desktop/EnglishChats1"

spark_df.coalesce(25).write.mode('append').parquet(output_directory)

#This completes Pre-Processing of the Dataset



from fastopic import FASTopic
from topmost.preprocessing import Preprocessing
import nltk
from nltk.corpus import stopwords

import re   # Import regular expressions

nltk.download('stopwords')
stop_words  = stopwords.words('english')
print(stop_words)

df['model'].value_counts()

temp = df['turn'].value_counts()
temp = temp.sort_index()
plt.bar(temp.index[0:20],temp[0:20])

plt.show()

model = FASTopic(20, save_memory=True, batch_size=9000)

def preprocess(textlist):
    if textlist is None:
      return ''
    else:
      for text in textlist:
        updated_content = []
        text = text.lower() #setting entire text to lower case
        #text = re.sub(r'\s{2,}', '', text).strip() #removing white spaces
        text = re.sub(r"<(.*?)>", ' ', text).strip() #removing html tags
        text = re.sub(r"[^A-Za-z\s']", '', text) #removing any other special characters other than ones in the square bracket
        words = text.split()
        #print(words , "\n")
        filtered_words = [word for word in words if len(word) > 2 and (word not in stop_words)]
        text = " ".join(filtered_words)
        text = re.sub(r"'", '', text).strip()
        #print(text)
      return text

text = "I     <want'> <to> vis'it the <city!> yy    #excited This is a short's test."
processed_content = preprocess(df.loc[0,'review_body'])
processed_content = preprocess(text)

print(processed_content)

topic_top_words, doc_topic_dist = model.fit_transform(df.review.values.tolist())





# Example of PI potentially harmful identifiable information, would be redacted
df[df['conversation_hash']=='0100842c3f4b3b4386b99326da477133']

df[df['redacted']==True].country.value_counts()

# Main Analysis - Focuses on Malta / Italy

# A majority of the journalist plagiarism articles seem to not have redacted tags,
# I need to search for them manually
dfcsv=df[(df['country']=='Malta') | (df['country']=='Italy')]
dfcsv=dfcsv[['conversation_hash','clean_interaction','turn','frequent_terms','rare_terms']]
print(dfcsv.shape)
#dfcsv.to_csv('dfcsv.csv', index=False)

#df = df[(df['country']=='Malta') | (df['country']=='Italy')]

df = df[(df['state']=='California')]

df.shape

searchterms = pd.Series(['rewrite','article','write'])
#dfcsv = searchterms.isin(df.rare_terms)

#dfcsv['article'] = dfcsv['rare_terms'].map(lambda x : sum(searchterms.isin(x)))
#dfcsv.article.value_counts()

df['article'] = df['rare_terms'].map(lambda x : sum(searchterms.isin(x)))
df.article.value_counts()

dfcsv=df[(df['article']>=2) & (df['state']=='California')]
dfcsv=dfcsv[['conversation_hash','full_interaction','redacted']]
print(dfcsv.shape)
dfcsv.to_csv('dfcsv.csv', index=False)

dfcsv=df[(df['state']=='Province of Taranto')]
print(dfcsv.shape)
dfcsv.to_csv('dfcsv.csv', index=False)
dfcsv

dfcsv=df[(df['article']==2)]
print(dfcsv.shape)
dfcsv.to_csv('dfcsv.csv', index=False)

searchterms = pd.Series(['rewrite','article','write'])
dfcsv=df[(df['country']=='Malta')]

dfcsv['article'] = dfcsv['rare_terms'].map(lambda x : sum(searchterms.isin(x)))
dfcsv.article.value_counts()

dfcsv = dfcsv[(dfcsv['article']>-1)]
print(dfcsv.shape)
dfcsv.to_csv('dfcsv.csv', index=False)

dfcsv=df[(df['country']=='Malta') & (dfcsv['article']>0)]
dfcsv.article.value_counts()

# With only 106 conversations, run time is only 1.5 minutes
# As opposed to 20 minutes for 2000 conversations

model = FASTopic(12, save_memory=True, batch_size=600)
topic_top_words, doc_topic_dist = model.fit_transform(dfcsv.clean_interaction.values.tolist())

doc_topic_dist

# For all Malta conversations
topic_top_words

fig = model.visualize_topic(top_n=10)
fig.show()

fig = model.visualize_topic_hierarchy()
fig.show()

fig = model.visualize_topic_weights(top_n=20, height=500)
fig.show()

# Older topic weights figure for Malta, seems like it does change a bit, there's some random seeding changes
fig = model.visualize_topic_weights(top_n=20, height=500)
fig.show()

model = FASTopic(8, save_memory=True, batch_size=600)
topic_top_words, doc_topic_dist = model.fit_transform(dfcsv.clean_interaction.values.tolist())



topic_top_words

fig = model.visualize_topic(top_n=10)
fig.show()

fig = model.visualize_topic_hierarchy()
fig.show()

fig = model.visualize_topic_weights(top_n=20, height=500)
fig.show()

dfcsv = df[(df['article']>=0) & (df['turn']<4)]
dfcsv=dfcsv[['conversation_hash','clean_interaction']]

dfcsv2=dfcsv.iloc[0:int(dfcsv.shape[0]/2)]
dfcsv=dfcsv.iloc[(int(dfcsv.shape[0]/2)+1):]

dfcsv.shape[0]

del df0,df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14

dfcsv2.shape

# Not enough RAM for several previous runs with larger parameters,
# Even so any reasonable configuration for just the state of California crashes
# due to RAM. Changed focus to Italy/Malta, reduced to 2000 batch size and 12 topics
# And analyzing only half of this 7000 size dataset at a time
# Still takes 20 minutes to process

model = FASTopic(12, save_memory=True, batch_size=410)
topic_top_words, doc_topic_dist = model.fit_transform(dfcsv.clean_interaction.values.tolist())


# Absolute largest model input size can be 2000 conversations, works much faster
# and better demo with 30-100 conversations queried by simple keyword and doing
# topic modelling solely within these search results



topic_top_words

fig = model.visualize_topic(top_n=10)
fig.show()

fig = model.visualize_topic_hierarchy()
fig.show()

# Conversations size = 2000, processing time 20 minutes

model = FASTopic(12, save_memory=True, batch_size=410)
topic_top_words, doc_topic_dist = model.fit_transform(dfcsv2.clean_interaction.values.tolist())

fig = model.visualize_topic(top_n=10)
fig.show()

topic_top_words

['che double int del una codice questo else bool false return true multiplier api essere',
 'div rem camping display href img src padding hover website backgroundcolor placeholder alt css https',
 'hello assist soap jms socket jakarta userid today date protocol jmstemplate worksariopen atomic sorry ratelimiterhandlerfilterfunction',
 'measure claim cell vector outcome cells ignore scientific frequency patients table oral extract format magnitude',
 'prompt description system service product users self type private detailed image server user prompts void',
 'happiness ophelia gas illegal american fantasy blockchain offgrid martial english amish waste countries psiphon modern',
 'rdp administrador nat administrator username channel address password giveaway contact present specification subscribers giveaways meblackcyberpiratesofficial',
 'torch packetid tensor nodeid pytorch putpacketid batch std output edgecost packetprocessingtime narra cloudcost loss narrb',
 'italy italian guarantee audio financial cryptocurrency bitcoin therapy september august sector bank roman medical companies',
 'song album melody apocalypse littlegpt sound michone fingers sunset distant birthday chocolate bloom zombie today',
 'clementine luis max john play says nods smile ill fire role looks mimi sumire yeah',
 'the and for that with you not this are can from his but its your']











